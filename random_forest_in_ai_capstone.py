# -*- coding: utf-8 -*-
"""Random Forest in AI Capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HUDDiIUZJTbRaMsvy1HfPE0S7LxZDZji
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import pylab as pl
import numpy as np
import scipy.optimize as opt
from sklearn import preprocessing
# %matplotlib inline
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
import seaborn as sns

df_delay= pd.read_csv('/content/drive/MyDrive/AI Capstone Dataset/DelayData.csv')
df_cycle= pd.read_csv('/content/drive/MyDrive/AI Capstone Dataset/CycleData.csv')
df_location= pd.read_csv('/content/drive/MyDrive/AI Capstone Dataset/LocationData.csv')

# Check for missing values
print("\n\nMissing Values of CycleData:\n")
print(df_cycle.isnull().sum())
print("\n\nMissing Values of DelayData:\n")
print(df_delay.isnull().sum())
print("\n\nMissing Values of LocationData:\n")
print(df_location.isnull().sum())

# Remove missing values
df_cycle = df_cycle.dropna(subset=['Fuel Used'],axis=0)
df_cycle = df_cycle.dropna(axis=1)
df_delay = df_delay.dropna(axis=1)
df_location = df_location.dropna(axis=1)

import matplotlib.pyplot as plt
# Check for missing values
print("\n\nMissing Values of CycleData:\n")
print(df_cycle.isnull().sum())
print("\n\nMissing Values of DelayData:\n")
print(df_delay.isnull().sum())
print("\n\nMissing Values of LocationData:\n")
print(df_location.isnull().sum())

# Check first rows on dataframe
print(df_cycle.head())
print(df_delay.head())
print(df_location.head())

##OEE Calculation
oee = df_cycle[['AT Available Time (iMine)', 'Down Time', 'iMine Operating Hours', 'OPERATINGTIME (CAT)', 'Idle Duration']]
oee['Availability'] = ((oee['AT Available Time (iMine)'] - oee['Down Time']) / oee['AT Available Time (iMine)']) * 100
oee['Performance'] = ((oee['OPERATINGTIME (CAT)'] - oee['Idle Duration']) / oee['OPERATINGTIME (CAT)']) * 100
oee['Quality'] = ((oee['iMine Operating Hours'] - oee['Down Time']) / (oee['Down Time'] + oee['Idle Duration'])) * 100
oee['oee'] = oee['Availability'] * oee['Performance'] * oee['Quality']

##OEE Calculations
def calculate_oee():
    return oee[['AT Available Time (iMine)', 'Down Time', 'iMine Operating Hours', 'OPERATINGTIME (CAT)',
                'Idle Duration', 'Availability', 'Performance', 'Quality', 'oee']]

##Print OEE Calculations
print(calculate_oee())

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

X = df_cycle.drop('Fuel Used', axis=1)  # Features
y = df_cycle['Fuel Used']  # Target variable
# Take the object var only and change to int type
object_columns = X.select_dtypes(include=['object']).columns
label_encoders = {}
for col in object_columns:
    label_encoders[col] = LabelEncoder()
    X[col] = label_encoders[col].fit_transform(X[col])
#print(X.dtypes)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)
print(y.head())

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Initialize the Random Forest regressor

rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train, y_train)

# Make predictions on the test data
predictions = rf_regressor.predict(X_test)

# Calculate accuracy
mse = mean_squared_error(y_test, predictions)
print("Mean Squared Error:", mse)

from sklearn.linear_model import LinearRegression
# X_train, X_test, y_train, y_test are already defined so we train our model
model= LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

#Create random samples of n =1000
sample_n = np.random.choice(len(y_test), size=1000, replace=False)

# Visualize actual vs predicted values

plt.figure(figsize=(8, 6))
plt.scatter(y_test.iloc[sample_n], y_pred[sample_n], color='blue')  # Use sampled data points
plt.plot(y_test, y_test, color='red', linestyle='--')  # Plotting the diagonal line for reference
plt.title('Actual vs Predicted Fuel Used')
plt.xlabel('Actual Fuel Used')
plt.ylabel('Predicted Fuel Used')
plt.show()

from sklearn.metrics import mean_squared_error, r2_score

# Calculate MSE
mse_linear = mean_squared_error(y_test, y_pred)

# Calculate R^2 score
r2_linear = r2_score(y_test, y_pred)

print("Linear Regression Model:")
print("MSE:", mse_linear)
print("R^2 Score:", r2_linear)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Define a smaller parameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [5, 10],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
}

# Reduce the number of iterations
n_iterations = 5

# Define the sample size
sample_size = 500

mse_results = []
feature_importances = []

for iteration in range(n_iterations):
    # Randomly sample the data
    random_indices = np.random.choice(len(X), size=sample_size, replace=False)
    X_sampled = X.iloc[random_indices]
    y_sampled = y.iloc[random_indices]

    # Split the sampled data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=iteration)

    # Initialize the Random Forest regressor
    rf_regressor = RandomForestRegressor(random_state=42)

    # Initialize RandomizedSearchCV with a smaller number of iterations
    random_search = RandomizedSearchCV(estimator=rf_regressor, param_distributions=param_grid,
                                       n_iter=4, cv=5, verbose=2, random_state=42, n_jobs=-1)

    # Fit the RandomizedSearchCV
    random_search.fit(X_train, y_train)

    # Get the best model
    best_rf_regressor = random_search.best_estimator_

    # Make predictions on the test data
    predictions = best_rf_regressor.predict(X_test)

    # Calculate mean squared error
    mse = mean_squared_error(y_test, predictions)
    mse_results.append(mse)

    print(f"Iteration {iteration + 1}/{n_iterations} - MSE: {mse}")

       # Get feature importances
    feature_importances.append(best_rf_regressor.feature_importances_)

# Print average MSE over all iterations
print("Average MSE:", np.mean(mse_results))

# Plot distribution of MSE
plt.figure(figsize=(8, 6))
sns.histplot(mse_results, kde=True, bins=5)
plt.title('Distribution of MSE')
plt.xlabel('MSE')
plt.ylabel('Frequency')
plt.show()

# Plot feature importances

mean_feature_importances = np.mean(feature_importances, axis=0)
sorted_indices = np.argsort(mean_feature_importances)[::-1][:5]  # Selecting only the top 5 features
plt.figure(figsize=(10, 6))
sns.barplot(x=mean_feature_importances[sorted_indices], y=X.columns[sorted_indices], palette="viridis")
plt.title('Top 5 Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()

from sklearn.metrics import r2_score

# Calculate R^2 for each iteration
r_squared_results = []

for iteration in range(n_iterations):
    # Make predictions on the test data
    predictions = best_rf_regressor.predict(X_test)

    # Calculate R^2
    r_squared = r2_score(y_test, predictions)
    r_squared_results.append(r_squared)

    print(f"Iteration {iteration + 1}/{n_iterations} - R^2: {r_squared}")

# Print average R^2 over all iterations
print("Average R^2:", np.mean(r_squared_results))

# Print average MSE over all iterations
print("Average MSE:", np.mean(mse_results))